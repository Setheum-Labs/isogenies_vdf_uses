\documentclass{article}

\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{unicode}
\usepackage{hyperref}


\newtheorem*{rawnamedtheorem}{\therawnamedtheorem}
\newcommand{\therawnamedtheorem}{\error}
\newenvironment{namedtheorem}[1]{\renewcommand{\therawnamedtheorem}{#1}
   \begin{rawnamedtheorem}}
  {\end{rawnamedtheorem}}

\newcommand{\Z}{\mathbb{Z}}

\newcommand\mathperiod{.}
\newcommand\mathcomma{,}


\title{Why VDFs from isogenies are so cool}
\author{Jeff, Luca}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We make a compelling argument for 

We believe that both algorithmic and hardware optimisations for an
isogeny VDF yield wide ranging benefits that go far beyond improving
RSA speeds.

\end{abstract}

\section{Intro}


- Isogenies vs RSA
- Isogenies vs Class groups
- Isogenies vs the rest





\section{Performance}

As a security assumption, we know some bound $A$ such that if the
fastest honest evaluator can evaluate our VDF in time $T$ then no
adversary can evaluate our VDF in time $T/A$.  
We always have some lower bound on $T/A$ in practice, so smaller
$A$ makes evaluating the VDF cheaper.

We choose VDF designs to create a cost divide among performance
optimisations.  We want some reasonable optimisations to have
relatively inexpensive development and deployment costs, while
all other optimisations look cost prohibitive, even for well funded
adversaries.

As an example, we do tentatively want hardware optimisations for
any deployed VDF, probably an FPGA or maybe even an ASIC, while we
exclude exotic computation techniques like super-conducting computing.

At the same time, we require any VDF to track newly discovered
optimisations as well.  We should thus favor VDFs for which expected
optimisations interact well with research and other applications.
In this vein, we worry about the implications of optimisations that
benefit only the VDF use case, including ASICs.

\subsection{Hardware}

An isogeny VDF works by evaluating isogenies between elliptic curves
using Velu's formulas.  In these calculations, we work with elements
in a prime field $\Z/p\Z$ and some low degree field extensions. 
We need a prime $p$ roughly half the size of a 3072 but RSA composite
with a similar 128 bit security level, or about 3/4ths the size of the
current 2048 bit RSA VDF proposal with 112 bits of security.

At present, RSA based VDFs appear simplest to implement in hardware
because they only require a squaring operation. % TODO: True??
% Justin Drake https://ethresear.ch/t/minimal-vdf-randomness-beacon/3566 or maybe something else from https://vdfresearch.org
In the isogeny VDF, we need more involved computations with these
large field elements than merely the squarings required by RSA. 

Yet, there are wide ranging benefits to fast arithmetic in large
prime fields, or even full isogeny calculations.  We must target
FPGAs first anyways, so our development effort would therefore
benefit other important cryptographic protocols:
 
Recursive zkSNARKs need similarly sized pairing friendly fields at
the 128 bit security level.  (TODO: Citation)
There remain diverse opinions on field sizes for pairning friendly
curves for use even with non-recursive zkSNARKs, with some growing
largish, but regardless prover and verifier time for zkSNARKs stand
to benefit even over conventionally sized fields.  

Isogenies-based key exchanges like CSIDH \cite{CSIDH} need somewhat
smaller fields.  CSIDH supports authentication and long-term public
keys better than lattice-based key exchanges. 
Also, there are low-latency circuit-based anonymity protocols like Tor,
for which lattice-based key exchange provide post-quantum security,
but such protocols are vulnerable to correlation attacks. 
Avoiding correlation attacks requires more asynchronous cryptography,
like in the Sphinx mixnet packet format, but CSIDH fits such protocols
much better than lattice schemes.  Improved CSIDH performance could
thus dramatically improve routing in future anonymity technologies.

Isogenies schemes like CSIDH remain an interesting long-term candidate
for small post-qauntum signatures and VRFs, although current schemes
like CSI-FiSh \cite{CSI-FiSh} has no advantages over say lattice-based
schemes like Dilithium \cite{Dilithium}
% https://www.esat.kuleuven.be/cosic/csi-fish-a-new-record-class-group-computation-and-fast-isogeny-based-signatures/
% https://pq-crystals.org/dilithium/index.shtml

We think arithmetic optimisations, including FPGA designs, that favour
all these use cases together would become more accessible to all users,
and thus better track technological advances.

We also note that RISC-V and ARM chips with FPGA coprosessors exist
now, which although questionable choices for VDF evaluation, sound
like excellent options for the above use cases.
% vaguely related: https://www.youtube.com/watch?v=4hq4yiVCopU#t=28m

Among likely hardware implementation options, 
we foresee no pressing need to favor ASICs over FPGAs because highly
optimised ASICs are only about 3-4 times faster than high speed FPGAs.
% to \cite  http://www.eecg.toronto.edu/~jayar/pubs/kuon/kuontcad06.pdf via https://www.quora.com/What-are-the-differences-between-ASIC-and-FPGA?share=1

These estimates might increase for non-parallelizable computations
because an FPGA can occupy 30-40 times as much space though, or
20 times when an FPGA coprosessor for field arithmetic.
As a rule, an ASICs largest savings over an FPGA actually come in
power consumption, like 14 times or 7 times for pure an coprosessor
designs, and per unit costs.  We do not care about either
because only a handful of nodes evaluate VDFs.  

As discussed below, arithmetic can and should be parallelized,
but attempts to do so using GPUs have mostly failed due to GPUs poor
performance for the few non-parallel parts of reductions.  
We do note such work focussed largely on RSA not fields of specially
crafted large prime order though.

\subsection{Memory}

We accelerate isogenies VDFs by using intermediate curve coefficients
precomputed during a previous slower run, which depends upon memory
latency.  We suggest however that memory latency provides a latency
target for the arithmetic parallelization discussed below.

TODO:  Memory bandwidth estimates?  Where?

In principle, an attacker might recompute these coefficients just in
time.  Yet, these precomputations have greater circuit depth, requiring
$O(T \log p)$ time as opposed to the $O(T)$ required for VDF evaluation
\cite[pp. 21]{IsogeniesVDF}.  At present \cite[pp. 15]{IsogeniesVDF} 
suggest $\log p \approx 1500$, but this can easily be increased.
An attacker must also do this computation in parallel, which further
increases their overhead.

Attackers who cannot keep these precomputations on the same chip remain
limited to the memory bus speed, and hence the speed of light. 
There remains a risk of attackers adding several independent busses to
different memory blocks, but this sounds limited by space.

We consider this extremely promising because industrial applications
like machine learning also befit form FPGAs with high memory bandwidth.
Indeed FPGAs with memory bandwidth comparable to Intel Xeons have
existed since 2015, and 
% https://www.achronix.com/achronix-introduces-the-highest-fpga-memory-bandwidth-pcie-acceleration-board-for-data-center-applications/
modern designs incorporate arithmetic circuits directly.
% https://www.achronix.com/product/speedster7t/
% Intel has similar equipment, but not sure the links.
These are admittedly highly specialised machines, but they resemble
conventional computers far more closely than an ASICs.
We think their prevalence in industry should therefore increase
confidence in the VDF latency dramatically, especially against
exotic attackers, like super-conducting computing.

\subsection{Arithmetic}

There is an extensive literature about parallelizing modular powers
for RSA by representing numbers using the Chinese remainder theorem,
known as a residue number systems, like \cite{RNSonGPU07,Moss07,Araujo12}.
Among the strongest asymptotic results comes from \cite{BernsteinSorenson07}:

\begin{namedtheorem}{Theorem 8.1 of \cite{BernsteinSorenson07}}
There is an algorithm for the Common CRCW PRAM that, given the binary
representations of positive integers $x$, $e$, and $m$, of total bit
length $n$, computes the binary representation of $x^e \mod m$ in time
$O_\epsilon(n / \log \log n)$ using $O(n^{2+\epsilon})$ processors.
\end{namedtheorem}
% https://cr.yp.to/antiforgery/meecrt-20060914-ams.pdf


We know for example that addition modulo $N$ should have running-time
$O(\log \log \log N)$ with an optimal residue base, but only
running-time $O(\log \log N)$ with a more natural residue base of only
Mersenne primes \cite[\S4.6]{Parhami}.
There are explanation in \cite{RNSonGPU07} and \cite{Araujo12} of
doing Montgomery modular multiplication, and thus squarings, over
two distinct residue bases. 
% also https://pdfs.semanticscholar.org/0fef/478462c8f21e75364ca767b1835f3f05f267.pdf



We think these reduces confidence in any particular ASIC for RSA
squarings, especially if finding the optimal residue base for a
particular modulous proves interesting.  Also, residue number systems
require far more silicon, likely increasing ASIC costs.

In the isogenies VDF by comparison, we choose the prime field $\Z/p\Z$
of definition for our curves so that $p+1$ has a large prime factor
and a large power of two.  
We can therefore have more confidence that our arithmetic operations
provide relatively optimal performance \cite{????}.  In other words,
we know relatively optimal residue bases for working in $\Z/p\Z$ \cite{????}.

TODO: Is this correct Luca?  Explain further.  
TODO: Do Velu's formula's need anything that makes using RNS hard?

An isogenies-based VDF instead risks mathematical or algorithmic
advancements in evaluating Velu's formulas, which initially sound
much more significant than arithmetic optimisations.  We suspect
this risk might actually be lower however for several reasons:

We expect significant advancements would arise by academics improving
Velu's formulas for specialised field and curve paramaters.
Initially, the vulnerable paramaters would usually differ from the
paramaters used by our VDF, which translates into time to replace
the VDF paramaters, if appropriate.  

As above, we observe that CSIDH \cite{CSIDH} benefits enormously from
improving Velu's formulas, which translates into faster post-quantum
authentication and anonymous routing, and maybe post-quantum signatures.

\subsection{Quantum annoyance}

We say a signature scheme, VRF, or VDF is {\em quantum annoying} if
an adversary with a quantum computer can forge signatures, compute
outputs without the public key, or compute outputs very quickly,
but they must utilize the quantum computer for each separate attack.
In other words, a quantum computer cannot break the public key or
paramaters once to provide information with which these attacks can
be run on a classical computer. 

As implemented, the class group VDF design is quantum annoying because
new class groups with different orders get constructed for each input.

There is a VRF and signature scheme similar to our VDF in which the
isogeny $φ$ gets kept secret, and runs quickly, and the public key
is the curve $X$ and $φ G_1$.  Now our output $\sigma = φ* H_2(m)$
acts as signature and the verification equation is 
$$ e_X( φ G_1, H_2(m) ) = e_Y( G_1, \sigma ) \mathperiod $$

Now our isogenies-based VDF construction and this VRF and signature
scheme are quantum annoying because the adversary cannot break the
isogeny itself from merely $φ$, but can find a discrete log for each
input point $H_2(m)$.

There are some properties of BLS signatures that carry over to this
quantum annoying VRF like distinct message aggregation.  
Its signatures are far smaller than any known post-quantum signature
scheme, while also providing competitive verifier performance.  
We think quantum annoyance could provide an important stepping stone
while the threat from quantum computers increases.  In particular,
there are many consensus protocols in which VRFs run frequently and
quickly, thus increasing the confidence derived from quantum annoyance.  

We note that deterministic hash-based signatures like Sphincs should
give fully post-quantum VRFs, but their extreme size makes them
unwieldy for consensus protocols.  We hope work on isogenies-based
signatures eventually yields more compact post-quantum VRFs too.
We do not know if lattice-based VRFs are even possible.


\section{Trusted setup}

There are several VDF designs currently under consideration, but
our two most serious candidates require a group $G$ of unknown order.
In $G$, these designs first repeatedly square some distinguished
element, while saving check point powers, and then produce a proof
that the squarings were done correctly, using the proof strategies
of either Pietrzak \cite{Pietrzak} or Wesolowski \cite{Wesolowski}.

In these proposals, the groups $G$ of unknown order has thus far been
either the class group of an imaginary quadratic order, or else an
RSA group $\Z/n\Z$ with $n = p q$ known but $p$ and $q$ unknown.

We do not require any trusted setup for the class group approach.
In fact, the group itself is normally constructed from the input,
thus avoiding precompute attacks and providing quantum annoyance.  

There is however a tricky highly synchronous MPC required to produce
an RSA modulus $n = p q$ with $p$ and $q$ unknown.  At minimum, such
a synchronous trusted setup limits participation and casts doubt on
the system's security.  

TODO: Comment on the RSA trusted setup proposed by Abhi Shelat at Ligero, as explained in ligeroRSA.pdf and  \url{https://www.youtube.com/watch?v=RwrJXO_ecRI&list=PLaM7G4Llrb7y075mVXGmSABDP9Nb_PsBq&index=7}

Isogenies VDFs require a trusted setup too because the distinguished
starting curve admits more optimisations when evaluating isogenies:
TODO: Luca please explain better.

In this trusted setup, we perform a random walks through curve $E_i$
around the specified isogeny volcano.  In sequence, each participants
producing a secret isogeny $φ_i : E_i \to E_{i+1}$ and publishes
$E_{i+1}$ along with curve points $U,V \in E_i$ and $X,Y \in E_{i+1}$
such that $e_{E_i}(U,V) = e_{E_{i+1}}(X,Y)$.  

Anyone can verify the whole trusted setup by checking all these
pairing equations.  We cannot aggregate these pairing computations
in verification, due to working on different curves.  Yet, verification
costs only two pairings per participants, which sounds typical.

There is no synchronous MPC in this trusted setup, meaning participants
could join flexibly over an extended period of time.  We could even
resume the trusted setup anytime in the future, thereby expanding the
participant set.  After doing so, the VDF should be rerun to
regenerate its paramaters of course.

We further note that avoiding this trusted setup for isogeny starting
curves remains an active research area independent of VDFs. 
In particular, there is considerable interest in being able to hash to
{\it a} curve on an isogeny volcano.


\section{Proofs}

We foresee VDFs being applied primarily for collaborative randomness
in consensus protocols.  

\subsection{Proof ownership}

These consensus protocols commonly reward the VDF evaluator for the
CPU time, but this risks another node stealing the reward by claiming
it evaluated the VDF.  Any two evaluators produce the same VDF output
of course, but we can express ownership of the output by using
information from intermediate stages without touching the output itself.

As noted above, the VDF designs under consideration use a group $G$
of unknown order.  In $G$, these designs first repeatedly square
some distinguished element, while saving check point powers, and
then produce a proof that the squarings were done correctly, using
the proof strategies of
 either Pietrzak \cite{Pietrzak} or Wesolowski \cite{Wesolowski}.

In both designs, there is a Fiat-Shamir transform that hashes the
input $a$ and output $b = a^(2^T)$ together to select one or more
intermediate stages to reveal for the proof.  In both we simply
including the evaluator's public key into this hash to produce a
proof owned by that one evaluator.  Actually doing this requires
adjusting some security paramaters slightly, but doing so should not
harm performance. 

An isogeny VDF presents a minor challenge here because the proof
is only the output.  We therefore add a separate ownership proof
consisting of $x = o φ_{1/2} G_1$ and $y = o^{-1} φ_{1/2}* H_2(m)$
where $O = o G_1$ is the evaluator's public key and
$φ_{1/2} : Y \to Z$ and $φ_{1/2}* : X \to Z$ are isogenies to
the midpoint curve $Z$ half way between $X$ and $Y$.
We may verify the public key's correctness by checking
$e_X(O, G_2) = e_Z(x, G_2)$ and verify the proofs correctness by
checking
$$ e_X( φ G_1, H_2(m) ) = e_Z( φ_{1/2} G_1, H_2(m) ) = e_Y( G_1, φ_{1/2}* H_2(m) ) \mathperiod $$

% Classical proof: (x₁, x₂, ...) = H(a,b)
% Ownership proof: (x₁, x₂, ...) = VRF_o(a,b) ?= H(a | b | oH'(a|b) )

\subsection{Proof composition}

We require at least one party evaluates the VDF and publishes the output, and proof if applicable.  Yet, we improve efficiency when as few parties as possible do so.  We thus want participants to know when they should spend resources on VDF evaluation.
% As a non-example, the caucus leader election protocol of fantômette \cite{fantomette} requires that every participant a run their VDF almost all the time. 

There are similar concerns that adversaries who reliably computes the VDF fastest can impact the protocol by revealing their output late.  We reduce the severity by not tuning the VDF delay automatically but instead manually tuning with experiments on known hardware.  Yet, adversaries increasing delays remains a significant threat.

We therefore propose that VDFs be evaluated in stages to provide live information about evaluation progress.  In this way, an adversary cannot delay beyond one stage without additional nodes taking over.  

We could chain evaluations with any VDF design, but we require separate proofs for stages won by different evaluators, if using either Pietrzak or Wesolowski's strategies.  We expect vagaries from block production and finality to randomize the VDF winners enough to make proof size linear in the number of stages.  We might produce proofs that combine several stages, except this makes proof productions far slower.

Isogenies VDFs provide a dramatic savings here because the proof is only the output.
$$ e_X( φ G_1, H_2(m) ) = e_{X_{1/2}}( φ_{1/2} G_1, φ_{1/2}* H_2(m) ) = e_Y( G_1, φ* H_2(m) ) \mathperiod $$


\section{Time-lock puzzles}

% https://people.seas.harvard.edu/~salil/research/timelock.pdf

There is a classical time-lock puzzle based on RSA \cite{TLP} that
amounts to running the RSA based VDF in an RSA group for which the
puzzle creator knows the trap door $p$ and $q$ of the RSA composite
$n = p q$.  

\subsection{Parallel time-locks}

There are seemingly insurmountable problems with deploying an RSA
time-lock puzzle protocol:
We need a puzzle of sufficient value to be worth solving to prevent
catastrophic denial of service attacks, but almost any realistic use
cases fail this test.  In particular, RSA time-lock puzzles are
useless for important applications like vicary auctions and voting. 
Also, if we incentivise puzzle solving then our puzzle's creator can
hijack the rewards.  

We expect some advanced RSA trusted setup MPC would permit multiple
parties to encrypt messages to an eventual decryption, but doing this
sounds extremely complex.

Isogenies VDFs solve this problem far more elegantly though:
We regard the VDF evaluation $φ* H_2(m)$ as an identity-based secret
key for an identity-based encryption scheme.  In this analogy, our
identity is the VDF's seed $m$, while our master public key is $φ G$,
and our master secret key is the isogeny $φ*$ itself.

In concrete terms, we have a party, normally a peer-to-peer network,
that agrees upon and publishes our seed $m$ and starts computing our
VDF evaluation $φ* H_2(m)$.  We trust thisparty cannot delay publishing
$m$, meaning one honest peer contribute to $m$ less than $T/A$ before
the VDF starts running in the peer-to-peer network case.
After this VDF run seeded by $m$ commences, any party Alice may create
an ephemeral secret scalar $u$, compute $s_u = e_X ( u φ G_1, H_2(m) )$,
and sends a ciphertext $(u G_1, E_{s_u}(\ldots))$ to any party Bob.
After an evaluator computes and publishes the VDF evaluation
$φ* H_2(m)$ then Bob can compute $s_u = e_Y ( u G_1, φ* H_2(m) )$ and
decrypt this ciphertext.

Importantly, our parties Alice and Bob have no involvement with
running the VDF, beyond monitoring the agreement process for $m$,
including the time when $m$ became known.  We expect the amortised
costs to Bob to be zero, either because maybe participants run one
puzzle together, or more likely because our peer-to-peer networks
evaluates the VDF for another reasons, like unbiased randomness.
Alice cannot therefore exploit the time-lock puzzles for such bad
denial of service attacks.

These free parallel time-lock puzzles have numerous applications in
distributed systems.  We obtain almsot trivial protocols for voting
and vicary auctions that provide far stronger security that existing
protocols, which mostly depend upon threshold secret sharing.
As above, all participants simply encrypt their bid or vote to our
eventual VDF evaluation $φ* H_2(m)$.  

In the voting case, we easily obtain a secret balot using standard
techniques, like authenticating voters when their votes enter some
mixing service, or zero-knowledge proofs over Merkle trees, or
blind signature constructions, like rerandomizable certificates.

% e_X ( u φ G_1, H_2(m) ) = s = e_Y ( u G_1, φ* H_2(m) )
% pk: uG
% ct: E_s(msg)

TODO: Show that game is better

\subsection{Sequential time-locks}

An RSA time-lock puzzle can ostensibly be given some extremely long
running time, like years or decades, while creating a slow isogneny
requires extensive precomputation.  An isogeny VDF can recapture the
classical time lock puzzle game easily enough, but only by sacrificing
its parallel users:

As before, Alice first selects an ephemeral secret scalar $u$ and
obtains the initial VDF seed $m_{0,u}$.  Yet, now she iteratively
computes $s_{i,u} = e_X ( u φ G_1, H_2(m_i) )$ and chooses subsequent
VDF seeds $m_{i+1,u} = H(s_{i,u})$, and finally sends the ciphertext
$(u G_1, E_{s_{k,u}}(\ldots))$ to Bob.
After Bob known $m_{i,u}$ evaluates $φ* H_2(m_{i,u})$ then he can
compute $s_{i,u} = e_Y ( u G_1, φ* H_2(m_i) )$, and find the subsequent
VDF seed $m_{i+1,u} = H(s_{i,u})$.

As $m_{i,u}$ depends upon $u$ for $i>0$, we no longer obtain parallel
time-lock puzzles from this construction, which makes it an extreme
denial of service target.  Alice can however choose $k$ as large as
she likes, subject to doing $k$ pairings herself.
We note $m_{0,u}$ may originate with some trusted source, which
prevents some attacks on RSA time-lock puzzles, like Alice stealing
rewards from an evaluator.

\subsection{Private VDF}

We believe the Caucus component of Fantomette \cite[\S5]{Fantomette}
to be only currently published scheme that uses private VDF evaluation.
We recommend against protocols like Fantomette because many concurrent
private VDF evaluations incurs environmental costs.  At the same time,
there are novel security properties obtainable through private VDF
evaluation, and they remain dramatically cheaper than proof-of-work.

We expect private VDF evaluations would run a VDF on a VRF output,
like in Fantomette.  If however the running VDF really needed to
be kept secret then the isogeny $φ*$ could be kept secret too.

As noted above, our VDF output $φ* H_2(m)$ amounts to a BLS signature,
with a secret isogeny $φ*$ replacing the secret scalar.  So our map
$m \mapsto φ* H_2(m)$ could simultaneously be both a private VDF as
well as a VRF.  

A priori, {\em verifiable delayed random function} (VDRF) or time-lock
VRF requires some heavy zkSNARK proving that $φ* : Y \to X$ incurs the
required delay by being generated correctly, and the correctness of
$X$, $Y$, $φ G_1$, and any intermediate curves revealed.  
As our zkSNARK encodes arithmetic in the base field $\Z/p\Z$,
it should operate in another pairing friendly curve whose group order
is divisible by $p$, which makes arithmetic mod $p$ relatively efficient.
We could obtain such curves using the Cocks-Pinch method, which appears
feasible thanks to the running time being polynomial in $\log p$ \cite{ordinary_cocks_pinch}.
% TODO:  Is this running time correct?  Find citation for supersingular case.


\section{Flexibility}

Isogeny VDFs inherit much of the extreme flexibility BLS signatures
possess, mostly through the various BLS aggregation strategies.

We caution that our isogeny being between supersingular elliptic
curves makes the paring not of type III, which breaks assumptions
like the Symmetric External Diffie-Hellman (SXDH) \cite{ordinary_cocks_pinch}.

\subsection{Aggregated evaluation}

We adapt the simple distinct message aggregation strategy for
BLS signatures:  If our messages $m_i$ are all distinct, then the
BLS signatures $\sigma_{i,j} = p_j H_2(m_i)$ may trivially be
aggregated as $\sigma = \sum_{i,j} \sigma_{i,j}$ and verified by
checking that $e(G_1,\sigma) = \Pi_j e(P_j, \sum_i H_2(m_i))$,
without any concerns about relationships among the public keys
$P_j = p_j G_1$.  

We rarely see distinct message aggregation of BLS signatures, due to
poor performance of multiplication in the target group.  
Yet, aggregating distinct messages yields a handy techniques for
isogeny VDFs.  We expect VDF messages to always be distinct of course,
but beyond this we may aggregate VDF evaluation itself whenever the
isogenies coinside.

As a simplified example,
we consider two isogenies $φ_0 : Y \to X_0$ and $φ_1 : X_0 \to X$
so that $φ_0*$ and $φ_0* φ_1*$ are evaluation isogenies for  % Use \circ ?/
two VDFs.  If we now start with two input messages $m_0$ and $m_1$,
then we need only evaluate $φ_0*$ to compute the aggregate evaluation
$$ \sigma = φ_0*( H_2(m_0) + φ_1* H_2(m_1) )$$
and the aggregate verification equation
$$ e(G_1,\sigma) = e(φ_0 G_1, H_2(m_0)) e(φ_1 φ_0 G_1, H_2(m_1)) \mathperiod $$
In practice, we expect $\sigma_1 = φ_1*(H_2(m_1)$ would always be
published early, so any verifiers who wished to trust the evaluation
of $φ_1*$ need only check 
$e(G_1,\sigma) = e(φ_0 G_1, H_2(m_0) + φ_1* H_2(m_1))$

In the VDF security model, our adversary has a significant advantage
$A$ in evaluate the VDF, meaning they can evaluate the VDF if time
$T/A$ where $T$ is the VDF running time.  As such, contributions to
$m_i$ only ensure security if released by contributors less than
$T/A$ before the VDF starts running.  We should therefore favor all
contributions arriving quickly in bursts

Yet, integrating a VDF with proof-of-stake block production schemes
like Ouroboros Praos naturally favours taking $m$ to be a hash of
all block production VRF outputs throughout an entire epoch. 
In this approach, only blocks released during the final $T/A$ of the
epoch secure the VDF against an adversary with advantage $A$.
As compensation, we might run roughy $A$ VDFs in parallel for $A$
epochs delay each, but $A = 128$ or even $1024$ sound plausible if
honest VDF evaluators do not possess specialised hardware.

In our simple aggregation example, we created two entry windows,
first $m_1$ and later $m_0$ with duration $T_{m_1}$ and $T_{m_0}$
respectively, into the one VDF evaluation $φ_0*$ with a long running
delay $T_{φ_0*}$.  We separate the earlier window $m_1$ from the later
window $m_0$ by another VDF evaluation $φ_1*$ with a possibly shorter
delay $T_{φ_1*}$.  Ideally, these two entry windows might halve our
required number of parallel evaluations, thus reducing our cost by a
factor of two. 

Yet, we encounter timing troubles when attempting aggregate the
evaluations of $φ_0* φ_1* H_2(m_1)$ into $φ_0* H_2(m_0)$ though:  
If an adversary has an advantage $A' < A$ against $φ_0*$, then they
could apply it to $φ_1*$ as well, in which case they again need only
control all slots in $m_0$ to bias the VDF output.  
If $2 T_{φ_1*} \leq T_{m_0}$ then this yields $2 A T_{m_1} \leq T_{m_0}$,
making $T_{m_1}$ extremely short for large $A$.  

We instead propose that different evaluation threads be aggregated
opportunistically.  Any evaluator learns one of $φ_1* H_2(m_1)$ or
$H_2(m_0)$ first, so they could begin computing say $φ_0* H_2(m_0)$
but apply delays to synchronise with $φ_0* φ_1* H_2(m_1)$ when 
$φ_1* H_2(m_1)$ arrives.  In principle, such synchronisation might
be enforced with incentives.  We acknowledge however that such a
system requires delicate tuning that takes into account the
security concerns.  

\subsection{Threshold evaluation}

In a similar vein, there exist threshold VDF constructions that
provide security so long as {\it either} the VDF security assumptions
hold, {\it or} some threshold security assumption holds. 

In an epoch, the $j$th contributor aka block producer encrypts
threshold verifiable secret shares \cite{Schoenmakers99asimple} % Stadler96publiclyverifiable
of some curve point $H_2(m_j)$ with designated VDF evaluators.
At the end of the epoch, the $i$th VDF evaluator begins computing
their share of the VDF  $φ* \sum_i \lambda_i H_2(m_i)$, where
$\lambda_i = \Pi_{j \neq i} {j \over j-i}$ are the Lagrange coefficients.

We dislike sharing secrets with distinguished parties for numerous
reasons.  Yet now, our either $j$th contributor could publish $H_2(m_j)$
somewhat early, or preferably our $i$th evaluator could reveal some
partial computation early, enabling aggregation.  In either case,
we improve liveness assurances over user threshold randomness designs.

We leave designing useful threshold VDFs for future work, but emphasise
that isogenies VDFs enable trade offs unavailable with other VDF designs. 

TODO: RandHound vs RandHerd

\bibliographystyle{alpha}
\bibliography{isovdf}

\end{document}

