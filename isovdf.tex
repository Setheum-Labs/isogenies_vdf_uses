\documentclass{article}

\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{amsmath,amsthm}
\usepackage{unicode}

\title{Why VDFs from isogenies are so cool}
\author{Jeff, Luca}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We make a compelling argument for 
\end{abstract}

\section{Intro}


- Isogenies vs RSA
- Isogenies vs Class groups
- Isogenies vs the rest


\section{Quantum annoyance}



\section{Performance}

As a security assumption, we some bound $A$ so that if the fastest
honest evaluator can evaluate out VDF in time $T$ then no adversary
can evaluate our VDF in time $T/A$.  We always have some lower
bound on $T/A$ in practice, so smaller $A$ evaluating the VDF cheaper.

We choose VDF designs to create a cost divide among performance
optimisations.  We want some reasonable optimisations to have
relatively inexpensive development and deployment costs, while
all other optimisations look cost prohibitive, even for well funded
adversaries.

As an example, we tentatively want a hardware implementation for
any deployed VDF, while we rule out exotic computation techniques
like super-conducting computing.  Among the hardware implementation
options, we note that FPGAs do provide performance similar to ASICs
for some algorithms \cite{????},
so this might not require manufacturing custom ASICs.

At the same time, we require any VDF to track newly discovered
optimisations as well.  We should thus favor VDFs for which expected
optimisations interact well with research and other applications.
In this vein, we worry about optimisations that benefit only the VDF
use case, including ASICs.

An isogeny VDF works by evaluating isogenies between elliptic curves
using Velu's formulas.  In these calculations, we work with elements
in a prime field $\Z/p\Z$ and some low degree field extensions. 
We need a prime $p$ roughly half the size of a 3072 but RSA composite
with a similar 128 bit security level, or about 3/4ths the size of the
current 2048 bit RSA VDF proposal with 112 bits of security.

At present, RSA based VDFs appear simplest to implement in hardware
because they only require a squaring operation. % TODO: True??
% Justin Drake https://ethresear.ch/t/minimal-vdf-randomness-beacon/3566 or maybe something else from https://vdfresearch.org
In the isogeny VDF, we need more involved computations with these
large field elements than merely the squarings required by RSA. 

Yet, there are wide ranging benefits for doing real arithmetic in
large prime fields quickly.  There are several options that fit these
constraints, like an extension to an open hardware platform, maybe an
FPGA module for RISC-V \cite{????}, or even carefully parallelizing
field arithmetic for GPUs \cite{????}. 
% vaguely related: https://www.youtube.com/watch?v=4hq4yiVCopU#t=28m
If fast enough, then these approaches might track algorithmic and
technological development better. 

\subsection{Memory}

We accelerate isogenies VDFs by using intermediate curve coefficients
precomputed during a previous slower run.  As such, an isogenies VDF
either requires dramatically more hardware to recompute coefficients,
or more likely fetches them from memory.  

TODO:  Memory bandwidth estimates?

TODO:  Advantages and disadvantages of memory bandwidth vs computation? 

TODO:  Additional circuit complexity to recompute cefficients?

We cannot yet claim that an isogenies VDF can be memory hard,
but at minimum this makes exotic attackers less likely, including
super-conducting computing.

\subsection{Algorithms}

We choose the prime field $\Z/p\Z$ of definition for our curves so
that $p+1$ has a large prime factor and a large power of two.  

TODO: Is this correct Luca?  Explain further.

We can therefore be confidence that our arithmetic operations provide
relatively optimal performance.  In other words, we know relatively
optimal residue bases for working in $\Z/p\Z$ \cite{????}.

We also note that Velu's formulas require ???? which helps mitigate
any advantage from seeking parallelism through alternative residue
number systems.

TODO:  Luca, is this at all true?  Citations?

At the same time, there is an extensive literature about accelerating
RSA using residue number systems \cite{????}.
We know for example that addition modulo $N$ should have running-time
$O(\log \log \log N)$ with an optimal residue base, but only
running-time $O(\log \log N)$ with a more natural residue base of only
Mersenne primes \cite[\S4.6]{Parhami}.  
We believe this reduces confidence in an RSA ASIC for VDF evaluation
because implementing a residue number systems ion hardware require far
more silicon.  Yet, these may make RSA more amenable to GPUs.

TODO: Again, is this at all true? 

\subsection{Synergies??}

We believe that both algorithmic and hardware optimisations for an
isogeny VDF yield wide ranging benefits that go far beyond improving
RSA speeds.

We also noted (TODO) that an isogenies VDF construction also provides
a compact quantum annoying VRF and hence signature.  There are some
properties of BLS signatures that carry over to this quantum annoying
VRF, like distinct message aggregation.  It's signatures are far
smaller than any known post-quantum signature scheme too, while also
providing competitive performance.  We think quantum annoyance could
provide an important stepping stone while the threat from quantum
computers increases. In particular, there are many consensus protocols
in which VRFs run frequently and quickly, thus increasing the confidence
in quantum annoyance.  

We've noted above that arithmetic for an isogenies-based VDF sounds
broadly useful.

We assert such generic tools at least ``ethically offsets'' the
high cost for developing specialised hardware, even if it does not
bring additional financial backing.  In particular, there are several
important cryptographic projects that benefit: 

Recursive zkSNARKs need similarly sized pairing friendly fields at
the 128 bit security level.  (TODO)

Isogenies-based key exchanges like CSIDH need somewhat smaller fields.
There are low-latency circuit-based anonymity protocols, 
for which lattice-based key exchange provide post-quantum security,
but any such protocols are vulnerable to correlation attacks. 
Avoiding correlation attacks requires more asynchronous cryptography,
like in the Sphinx mixnet packet format, but CSIDH fits such protocols
much better than lattice schemes.  Improved CSIDH performance could
thus dramatically improve routing in future anonymity technologies.

CSIDH remains an interesting candidate for small post-qauntum signatures
too.  TODO: Luca SeaSign ???



\section{Trusted setup}

There are several VDF designs currently under consideration, but
our two most serious candidates require a group $G$ of unknown order.
In $G$, these designs first repeatedly square some distinguished
element, while saving check point powers, and then produce a proof
that the squarings were done correctly, using the proof strategies
of either Pietrzak \cite{Pietrzak} or Wesolowski \cite{Wesolowski}.

In these proposals, the groups $G$ of unknown order has thus far been
either the class group of an imaginary quadratic order, or else an
RSA group $\Z/n\Z$ with $n = p q$ known but $p$ and $q$ unknown.

We do not require any trusted setup for the class group approach.
In fact, the group itself is normally constructed from the input,
thus avoiding precompute attacks and providing quantum annoyance.  

There is however a tricky highly synchronous MPC required to produce
an RSA modulus $n = p q$ with $p$ and $q$ unknown.  At minimum, such
a synchronous trusted setup limits participation and casts doubt on
the system's security.  

TODO: Comment on the RSA trusted setup proposed by Abhi Shelat at Ligero, as explained in ligeroRSA.pdf and  https://www.youtube.com/watch?v=RwrJXO_ecRI&list=PLaM7G4Llrb7y075mVXGmSABDP9Nb_PsBq&index=7

Isogenies VDFs do require a nicersequential trusted setup because the
distinguished starting curve admit more optimisations when evaluating
isogenies:
TODO: Luca please explain better.

Our trusted setup produces a random walks through curve $E_i$ around
the specified isogeny volcano.  Each participants producing a secret
isogeny $φ_i : E_i \to E_{i+1}$ and publishes $E_{i+1}$ along with
curve points $U,V \in E_i$ and $X,Y \in E_{i+1}$ such that
$e_{E_i}(U,V) = e_{E_{i+1}}(X,Y)$.  

Anyone can verify the whole trusted setup by checking all these
pairing equations.  We cannot aggregate these pairing computations
when verifying, which makes verification time linear in the number
of participants, but this sounds typical.

We could restart the trusted setup anytime, thereby expanding the
participant set.  After doing so, the VDF should be rerun to
regenerate its paramaters.

We further note that avoiding this trusted setup for isogeny starting
curves remains an active research area independent of VDFs. 
In particular, there is considerable interest in being able to hash to
{\it a} curve on an isogeny volcano.


\section{Proofs}

We foresee VDFs being applied primarily for collaborative randomness
in consensus protocols.  

\subsection{Proof ownership}

These consensus protocols commonly reward the VDF evaluator for the
CPU time, but this risks another node stealing the reward by claiming
it evaluated the VDF.  Any two evaluators produce the same VDF output
of course, but we can express ownership of the output by using
information from intermediate stages without touching the output itself.

As noted above, the VDF designs under consideration use a group $G$
of unknown order.  In $G$, these designs first repeatedly square
some distinguished element, while saving check point powers, and
then produce a proof that the squarings were done correctly, using
the proof strategies of
 either Pietrzak \cite{Pietrzak} or Wesolowski \cite{Wesolowski}.

In both designs, there is a Fiat-Shamir transform that hashes the
input $a$ and output $b = a^(2^T)$ together to select one or more
intermediate stages to reveal for the proof.  In both we simply
including the evaluator's public key into this hash to produce a
proof owned by that one evaluator.  Actually doing this requires
adjusting some security paramaters slightly, but doing so should not
harm performance. 

An isogeny VDF presents a minor challenge here because the proof
is only the output.  We therefore add a separate ownership proof
consisting of $x = o φ_{1/2} G_1$ and $y = o^{-1} φ_{1/2}* H(m)$
where $O = o G_1$ is the evaluator's public key and
$φ_{1/2} : Y \to Z$ and $φ_{1/2}* : X \to Z$ are isogenies to
the midpoint curve $Z$ half way between $X$ and $Y$.
We may verify the public key's correctness by checking
$e_X(O, G_2) = e_Z(x, G_2)$ and verify the proofs correctness by
checking
$$ e_X( φ G_1, H(m) ) = e_Z( φ_{1/2} G_1, H(m) ) = e_Y( G_1, φ_{1/2}* H(m) ) \mathperiod $$

% Classical proof: (x₁, x₂, ...) = H(a,b)
% Ownership proof: (x₁, x₂, ...) = VRF_o(a,b) ?= H(a | b | oH'(a|b) )

\subsection{Proof composition}

We require at least one party evaluates the VDF and publishes the output, and proof if applicable.  Yet, we improve efficiency when as few parties as possible do so.  We thus want participants to know when they should spend resources on VDF evaluation.
% As a non-example, the caucus leader election protocol of fantômette \cite{fantomette} requires that every participant a run their VDF almost all the time. 

There are similar concerns that adversaries who reliably computes the VDF fastest can impact the protocol by revealing their output late.  We reduce the severity by not tuning the VDF delay automatically but instead manually tuning with experiments on known hardware.  Yet, adversaries increasing delays remains a significant threat.

We therefore propose that VDFs be evaluated in stages to provide live information about evaluation progress.  In this way, an adversary cannot delay beyond one stage without additional nodes taking over.  

We could chain evaluations with any VDF design, but we require separate proofs for stages won by different evaluators, if using either Pietrzak or Wesolowski's strategies.  We expect vagaries from block production and finality to randomize the VDF winners enough to make proof size linear in the number of stages.  We might produce proofs that combine several stages, except this makes proof productions far slower.

Isogenies VDFs provide a dramatic savings here because the proof is only the output.
$$ e_X( φ G_1, H(m) ) = e_{X_{1/2}}( φ_{1/2} G_1, φ_{1/2}* H(m) ) = e_Y( G_1, φ* H(m) ) \mathperiod $$


\section{Time-lock puzzles}

% https://people.seas.harvard.edu/~salil/research/timelock.pdf

There is a classical time-lock puzzle based on RSA \cite{TLP} that
amounts to running the RSA based VDF in an RSA group for which the
puzzle creator knows the trap door $p$ and $q$ of the RSA composite
$n = p q$.  

We foresee extensive issues with deploying their RSA time-lock puzzle
protocol:
We need a puzzle of sufficient value to be worth solving, which makes
RSA time-lock puzzles useless for voting and vicary auctions. 
If we incentivise puzzle solving then our puzzle's creator can hijack
the rewards.   

We expect some advanced RSA trusted setup MPC would permit multiple
parties to encrypt messages to an eventual decryption, but doing this
sounds extremely complex.

Isogenies VDFs solve this problem far more elegantly:  We regard the
VDF evaluation $φ* H(m)$ as an identity-based secret key for an
identity-based encryption scheme.  In this analogy, our identity is
the VDF's seed $m$, while our master public key is $φ G$, and
our master secret key is the isogeny $φ*$ itself.

In concrete terms, we first agree upon our seed $m$ and start computing
our VDF evaluation $φ* H(m)$.  At this point, any party can create an
ephemeral scalar $u$, compute $s_u = e_X ( u φ G, H(m) )$, and
send a ciphertext $(u G, E_{s_u}(\ldots))$.
After we compute the VDF evaluation $φ* H(m)$ then anyone knowing it
can compute $s_u = e_Y ( u G, φ* H(m) )$ and decode this ciphertext.

Importantly, these parties have no involvement with running the VDF,
beyond monitoring the timing with which $m$ became known.  As such,
we support vicary auctions and voting with an unlimited number of
participants, or numerous other use cases for distributed systems.

% e_X ( u φ G, H(m) ) = s = e_Y ( u G, φ* H(m) )
% pk: uG
% ct: E_s(msg)

TODO: Show that game is better


\section{Flexibility}

Isogeny VDFs inherit much of the extreme flexibility BLS signatures
possess, mostly through the various BLS aggregation strategies.

\subsection{Aggregated evaluation}

We adapt the simple distinct message aggregation strategy for
BLS signatures:  If our messages $m_i$ are all distinct, then the
BLS signatures $\sigma_{i,j} = p_j H_2(m_i)$ may trivially be
aggregated as $\sigma = \sum_{i,j} \sigma_{i,j}$ and verified by
checking that $e(G_1,\sigma) = \Pi_j e(P_j, \sum_i H_2(m_i))$,
without any concerns about relationships among the public keys
$P_j = p_j G_1$.  

We rarely see distinct message aggregation of BLS signatures, due to
poor performance of multiplication in the target group.  
Yet, aggregating distinct messages yields a handy techniques for
isogeny VDFs.  We expect VDF messages to always be distinct of course,
but beyond this we may aggregate VDF evaluation itself whenever the
isogenies coinside.

As a simplified example,
we consider two isogenies $φ_0 : Y \to X_0$ and $φ_1 : X_0 \to X$
so that $φ_0*$ and $φ_0* φ_1*$ are evaluation isogenies for  % Use \circ ?/
two VDFs.  If we now start with two input messages $m_0$ and $m_1$,
then we need only evaluate $φ_0*$ to compute the aggregate evaluation
$$ \sigma = φ_0*( H_2(m_0) + φ_1* H_2(m_1) )$$
and the aggregate verification equation
$$ e(G_1,\sigma) = e(φ_0 G_1, H_2(m_0)) e(φ_1 φ_0 G_1, H_2(m_1)) \mathperiod $$
In practice, we expect $\sigma_1 = φ_1*(H_2(m_1)$ would always be
published early, so any verifiers who wished to trust the evaluation
of $φ_1*$ need only check 
$e(G_1,\sigma) = e(φ_0 G_1, H_2(m_0) + φ_1* H_2(m_1))$

In the VDF security model, our adversary has a significant advantage
$A$ in evaluate the VDF, meaning they can evaluate the VDF if time
$T/A$ where $T$ is the VDF running time.  As such, contributions to
$m_i$ only ensure security if released by contributors less than
$T/A$ before the VDF starts running.  We should therefore favor all
contributions arriving quickly in bursts

Yet, integrating a VDF with proof-of-stake block production schemes
like Ouroboros Praos naturally favours taking $m$ to be a hash of
all block production VRF outputs throughout an entire epoch. 
In this approach, only blocks released during the final $T/A$ of the
epoch secure the VDF against an adversary with advantage $A$.
As compensation, we might run roughy $A$ VDFs in parallel for $A$
epochs delay each, but $A = 128$ or even $1024$ sound plausible if
honest VDF evaluators do not possess specialised hardware.

In our simple aggregation example, we created two entry windows,
first $m_1$ and later $m_0$ with duration $T_{m_1}$ and $T_{m_0}$
respectively, into the one VDF evaluation $φ_0*$ with a long running
delay $T_{φ_0*}$.  We separate the earlier window $m_1$ from the later
window $m_0$ by another VDF evaluation $φ_1*$ with a possibly shorter
delay $T_{φ_1*}$.  Ideally, these two entry windows might halve our
required number of parallel evaluations, thus reducing our cost by a
factor of two. 

Yet, we encounter timing troubles when attempting aggregate the
evaluations of $φ_0* φ_1* H_2(m_1)$ into $φ_0* H_2(m_0)$ though:  
If an adversary has an advantage $A' < A$ against $φ_0*$, then they
could apply it to $φ_1*$ as well, in which case they again need only
control all slots in $m_0$ to bias the VDF output.  
If $2 T_{φ_1*} \leq T_{m_0}$ then this yields $2 A T_{m_1} \leq T_{m_0}$,
making $T_{m_1}$ extremely short for large $A$.  

We instead propose that different evaluation threads be aggregated
opportunistically.  Any evaluator learns one of $φ_1* H_2(m_1)$ or
$H_2(m_0)$ first, so they could begin computing say $φ_0* H_2(m_0)$
but apply delays to synchronise with $φ_0* φ_1* H_2(m_1)$ when 
$φ_1* H_2(m_1)$ arrives.  In principle, such synchronisation might
be enforced with incentives.  We acknowledge however that such a
system requires delicate tuning that takes into account the
security concerns.  

\subsection{Threshold evaluation}

In a similar vein, there exist threshold VDF constructions that
provide security so long as {\it either} the VDF security assumptions
hold, {\ir or} some threshold security assumption holds. 

In an epoch, the $j$th contributor aka block producer encrypts
threshold verifiable secret shares \cite{Schoenmakers99asimple} % Stadler96publiclyverifiable
of some curve point $H_2(m_j)$ with designated VDF evaluators.
At the end of the epoch, the $i$th VDF evaluator begins computing
their share of the VDF  $φ* \sum_i \lamba_i H_2(m_i)$, where
$\lambda_i = \Pi_{j \neq i} {j \over j-i}$ are the Lagrange coefficients.

We dislike sharing secrets with distinguished parties for numerous
reasons.  Yet now, our either $j$th contributor could publish $H_2(m_j)$
somewhat early, or preferably our $i$th evaluator could reveal some
partial computation early, enabling aggregation.  In either case,
we improve liveness assurances over user threshold randomness designs.

We leave designing useful threshold VDFs for future work, but emphasise
that isogenies VDFs enable trade offs unavailable with other VDF designs. 

TODO: RandHound vs RandHerd


\end{document}
